# CPU 코어 간 컨텍스트 스위칭이 많다고 판단은 어떻게?
CPU 코어 간 컨텍스트 스위칭이 많다는 판단은 시스템의 성능 모니터링 툴과 운영체제의 통계 정보를 통해 알 수 있습니다. 컨텍스트 스위칭이 많다는 것은 CPU가 프로세스 간의 상태를 자주 전환해야 하므로 성능 저하가 발생할 가능성이 크기 때문입니다.

🔹 컨텍스트 스위칭 모니터링 방법
vmstat (Virtual Memory Statistics)

리눅스에서 vmstat 명령어를 사용하면 시스템의 컨텍스트 스위칭 관련 정보를 볼 수 있습니다. vmstat는 시스템의 메모리, 프로세스, CPU 상태를 모니터링합니다.
bash
vmstat 1
vmstat 출력의 주요 항목:
cs (context switches): 시스템에서 발생한 컨텍스트 스위칭의 수.
r (running processes): 실행 중인 프로세스 수.
b (blocked processes): 차단된 프로세스 수.
컨텍스트 스위칭이 많다는 것을 판단하려면 cs 값이 비정상적으로 높다면 그만큼 CPU 간의 컨텍스트 스위칭이 자주 발생한다고 볼 수 있습니다.

mpstat (Multiprocessor Statistics)

mpstat 명령어는 멀티코어 시스템에서 CPU 코어 간의 성능 통계를 제공합니다. 특히 각 CPU 코어의 사용률, 상태 등을 확인할 수 있습니다.
bash
mpstat -P ALL 1
mpstat의 주요 항목:
intr/s (interrupts per second): 하드웨어 인터럽트 및 소프트웨어 인터럽트의 수.
cpu (user, system, idle): 각 CPU 코어에서의 사용자 프로세스, 시스템 프로세스, 유휴 상태 비율.
높은 인터럽트 비율과 낮은 유휴 시간은 시스템이 많은 작업을 처리하고 있거나, 과도한 컨텍스트 스위칭을 유발하는 작업이 있을 가능성을 시사합니다.

pidstat

**pidstat**는 각 프로세스 별로 CPU 사용률, I/O 작업, 컨텍스트 스위칭 등을 추적할 수 있는 도구입니다.
bash
pidstat -w 1
pidstat -w는 각 프로세스가 하나의 CPU에서 다른 CPU로 전환되는 컨텍스트 스위칭 횟수를 추적합니다. 이 값이 비정상적으로 높다면, 프로세스 간의 자주 변경되는 CPU 상태를 알 수 있습니다.
dstat

**dstat**는 시스템의 다양한 리소스를 동시에 모니터링할 수 있는 도구입니다. 이를 통해 컨텍스트 스위칭, CPU 사용률, 메모리 사용량 등을 동시에 볼 수 있습니다.
bash
dstat -c --cpu --w
/proc 파일 시스템

/proc 디렉토리 내에서 컨텍스트 스위칭 정보를 확인할 수 있습니다.
/proc/stat: 시스템의 전체적인 컨텍스트 스위칭 정보를 제공합니다.
procs_running(실행 중인 프로세스 수)
procs_blocked(차단된 프로세스 수)
ctxt(전체 시스템에서 발생한 컨텍스트 스위칭 횟수)
/proc/[pid]/stat: 특정 프로세스의 컨텍스트 스위칭 통계를 확인할 수 있습니다.
예를 들어:

bash
cat /proc/stat | grep ctxt
이 명령은 시스템에서 발생한 전체 컨텍스트 스위칭 횟수를 출력합니다. 이 값이 비정상적으로 높다면 컨텍스트 스위칭이 많이 발생하고 있다는 신호입니다.

🔹 컨텍스트 스위칭이 많은 경우 나타나는 증상
CPU 과부하:

프로세스가 자주 상태를 전환해야 하므로, CPU가 과부하 상태가 되며 성능 저하가 발생할 수 있습니다.
메모리 캐시 미스 증가:

자주 컨텍스트 스위칭이 발생하면 캐시에서 데이터를 가져오는 빈도가 늘어나 캐시 미스가 증가할 수 있습니다. 이는 메모리 접근 시간이 길어지게 하여 성능을 떨어뜨립니다.
프로세스 응답 지연:

프로세스가 자주 중단되고 다시 시작되기 때문에, 응답 시간이 증가할 수 있습니다.
🔹 컨텍스트 스위칭 과다 발생 원인
많은 수의 프로세스 실행: 너무 많은 프로세스가 동시에 실행되면, 자주 CPU 간에 컨텍스트 스위칭이 발생할 수 있습니다.
비효율적인 멀티스레딩: 적절한 CPU 코어 할당 없이 프로세스가 계속해서 다른 코어로 이동하면 컨텍스트 스위칭이 자주 발생합니다.
I/O 작업: I/O 작업이 많으면, CPU가 기다리는 동안 컨텍스트 스위칭이 발생할 수 있습니다.
상호 배제(mutex, lock)와 같은 동기화 작업: 프로세스 간의 경쟁 상태나 자원 대기 시간으로 인해 자주 CPU가 변경될 수 있습니다.
🔹 컨텍스트 스위칭 과다 해결 방안
CPU Affinity 사용:

프로세스들이 특정 CPU에서 실행되도록 affinity를 설정하여 불필요한 코어 간 이동을 줄입니다.
프로세스 최적화:

필요 이상의 프로세스를 실행하지 않도록 하고, 멀티스레딩을 사용할 경우 스레드 수와 CPU 코어 수를 잘 맞추는 것이 중요합니다.
I/O 최적화:

I/O 작업이 많다면, 이를 최소화하거나 비동기 I/O 처리를 고려합니다.
Lock-Free 알고리즘 사용:

동기화가 필요한 작업에서 Lock-Free 알고리즘을 사용하여 자원 경쟁을 최소화합니다.
🔹 결론
컨텍스트 스위칭이 많다고 판단하는 방법은 vmstat, mpstat, pidstat, /proc/stat 등 다양한 도구와 명령어를 통해 확인할 수 있습니다. 이를 통해 컨텍스트 스위칭 횟수가 비정상적으로 높다면, CPU와 메모리 자원의 과도한 사용을 줄이기 위한 최적화가 필요하다는 것을 알 수 있습니다.


# cpu idle 이 95% 이상인데도 CPU 과부하가 나오는 경우
CPU idle이 95% 이상인데도 CPU 과부하가 발생하는 경우는 시스템의 자원 사용이 CPU 외에 다른 요소에 의해서 영향을 받을 때 발생할 수 있습니다. 즉, CPU는 유휴 상태에 있지만 메모리, I/O, 네트워크, 디스크와 같은 다른 자원들이 병목을 일으키고 있기 때문에 CPU 과부하처럼 보일 수 있습니다. 이런 현상은 다음과 같은 원인들로 발생할 수 있습니다:

1. I/O 병목 (Input/Output Bottleneck)
디스크 I/O, 네트워크 I/O, 혹은 기타 외부 장치와의 통신이 너무 느려서 시스템이 대기 상태에 빠지는 경우가 발생할 수 있습니다. 
CPU가 유휴 상태에 있을 때도, I/O 작업이 완료되기를 기다리며 시스템이 병목 상태에 빠질 수 있습니다.
예: 디스크에서 데이터를 읽거나 네트워크에서 데이터를 받기 위해 대기 중일 때, CPU는 작업을 처리하지 않지만, 시스템이 응답 속도에서 병목을 경험할 수 있습니다.
2. 메모리 부족 (Memory Bottleneck)
메모리 부족 또는 **스와핑(swap)**이 발생하면, 시스템이 디스크를 사용하여 메모리 부족을 보충하려고 시도하면서 성능 저하가 발생할 수 있습니다. 
이는 CPU 사용률이 낮더라도 시스템 응답 속도가 느려지거나 병목이 발생하는 상황을 만들 수 있습니다.
스와핑이 발생하면 메모리와 디스크 간에 빈번한 데이터 이동이 이루어지기 때문에 CPU는 상대적으로 유휴 상태여도 성능 저하가 심각할 수 있습니다.
vmstat를 사용하여 메모리 사용 상태를 확인하고, si (swap in), so (swap out) 값이 높다면, 스와핑이 발생하고 있다는 신호일 수 있습니다.
3. 컨텍스트 스위칭 과다
과도한 컨텍스트 스위칭이 발생할 때도 CPU는 상대적으로 유휴 상태로 보일 수 있습니다. 
이때 프로세스 간 빈번한 상태 전환이 이루어져, CPU는 각 프로세스를 처리하려고 하지만, 시스템 전체의 성능 저하를 초래할 수 있습니다.
컨텍스트 스위칭이 많으면 시스템 자원의 효율적인 사용을 방해하고, 결국 CPU의 사용 효율성을 떨어뜨려 과부하처럼 보일 수 있습니다.
4. 스케줄러 및 우선순위 문제
스케줄러가 적절한 CPU 자원 분배를 하지 못하거나, 프로세스의 우선순위가 잘못 설정되어 CPU가 일부 작업을 처리하는 데 비효율적일 수 있습니다. 
이 경우 CPU는 유휴 상태일 수 있지만, 다른 작업들이 실행되기 위해 대기하면서 성능이 저하될 수 있습니다.
5. 락 경합 (Lock Contention)
동기화 문제로 인해 다수의 프로세스나 스레드가 동일한 자원을 접근하려고 시도하는 경우, 락 경합이 발생합니다. 
이로 인해 프로세스가 블록되거나 대기 상태에 빠질 수 있으며, 실제로 CPU가 사용되지 않더라도 시스템은 비효율적으로 동작할 수 있습니다.
락 경합을 해결하지 않으면, CPU 사용률은 낮고 시스템 성능은 저하되는 상호 배제 문제에 직면할 수 있습니다.
6. 멀티스레딩/멀티프로세스 스케일링 문제
멀티스레딩 또는 멀티프로세스 애플리케이션이 CPU 코어에 잘 배분되지 않거나, 작업을 분배하는 데 비효율적일 경우, CPU 사용률은 낮지만 처리 속도가 느려지고, 시스템은 병목 상태에 빠질 수 있습니다.
예를 들어, 프로세스가 너무 많이 fork되거나, 각 스레드가 지나치게 자원을 차지하여 병목을 유발할 수 있습니다.
7. 네트워크 대기 시간 (Network Latency)
네트워크 대기 시간으로 인해 TCP/IP 프로토콜의 지연이나 패킷 손실 등이 발생하면, CPU는 대기 상태에 있어도 애플리케이션의 성능이 저하될 수 있습니다.
네트워크 관련 작업이 대기 중일 때 CPU는 사용하지 않지만, 애플리케이션의 응답 시간이 늘어나는 결과를 초래할 수 있습니다.
8. 저장소 성능 문제 (Storage Performance Issues)
디스크 속도나 스토리지 시스템이 느릴 경우, CPU는 대기 상태일 수 있지만 시스템 성능은 크게 저하될 수 있습니다. 
특히 디스크 I/O가 많은 작업이 있을 경우, CPU는 유휴 상태로 보이지만 디스크 작업이 병목이 되어 성능에 영향을 미칩니다.
🔹 이 문제를 해결하는 방법
iostat, vmstat, mpstat 사용:

디스크 I/O, 메모리, CPU 사용률 등을 확인하여 병목이 발생하는 자원을 파악합니다.
iostat와 vmstat로 디스크와 메모리 상태를 확인하고, 스와핑이 일어나는지, 디스크 대기 시간이 긴지 등을 점검합니다.
top 또는 htop 사용:

top 또는 htop 명령어를 사용하여 실시간 프로세스와 시스템 자원 사용을 모니터링하고, CPU 사용률 외에도 메모리와 I/O가 높은 프로세스를 확인합니다.
I/O 최적화:

디스크 및 네트워크 I/O가 병목의 원인이라면, 디스크 속도를 개선하거나 I/O 작업을 비동기식으로 처리할 수 있습니다.
메모리 최적화:

스와핑을 줄이기 위해 시스템에서 사용 가능한 메모리 용량을 늘리거나, 메모리 풀을 효율적으로 관리하여 메모리 병목을 해결합니다.
락 경합 및 스케줄링 개선:

락 경합을 피하고 스레드 또는 프로세스의 스케줄링을 효율적으로 조정하여 CPU 자원을 적절하게 배분합니다.
프로세스와 스레드 수 최적화:

프로세스 수나 스레드 수를 최적화하여 시스템의 자원 사용을 효율적으로 관리합니다.
🔹 결론
CPU idle이 95% 이상이라도 디스크 I/O, 메모리 부족, 락 경합, 네트워크 대기 시간 등 다른 자원에서 병목이 발생하면 CPU 과부하처럼 보일 수 있습니다. 
모니터링 도구를 사용하여 시스템의 병목 지점을 정확히 파악하고, 해당 자원을 최적화하는 방식으로 문제를 해결할 수 있습니다.


# affinity 할당하면 성능이 얼마나 올라갈것으로 예상되?
CPU affinity를 설정하면 성능 향상 효과가 있을 수 있지만, 그 효과는 다양한 요소에 따라 다를 수 있습니다. 특히, 성능 향상 정도는 다음과 같은 주요 요인에 의해 결정됩니다:

🔹 성능 향상 요인
CPU 캐시 지역성:

**캐시 지역성(Locality)**은 성능 향상의 중요한 요소입니다. 프로세스가 같은 CPU 코어에서 실행되면, 해당 코어의 **캐시(L1, L2, L3)**를 재사용할 수 있습니다. CPU는 캐시에서 데이터를 가져오는 것이 메인 메모리보다 훨씬 빠르므로, 프로세스가 자주 같은 코어에서 실행되면 캐시 미스가 줄어들어 성능이 향상됩니다.
CPU 코어 간 컨텍스트 스위칭 감소:

컨텍스트 스위칭이란 CPU가 다른 프로세스를 실행하기 위해 상태를 변경하는 작업입니다. CPU affinity를 설정하면 프로세스가 동일한 코어에서 계속 실행되므로, 불필요한 컨텍스트 스위칭이 줄어들어 성능이 개선됩니다.
높은 프로세스 수와 컨텍스트 스위칭 비율이 높은 경우 성능 향상이 더 크게 나타날 수 있습니다.
멀티스레딩 성능:

멀티스레딩을 사용하는 애플리케이션에서는 각 스레드가 같은 코어에서 실행되도록 설정하면 스레드 간의 캐시 공유와 메모리 접근 효율성이 향상될 수 있습니다.
NUMA(NON-UNIFORM MEMORY ACCESS) 최적화:

시스템이 NUMA 아키텍처를 사용할 경우, 서로 다른 CPU에 메모리가 분산되어 있습니다. 이때, CPU affinity를 올바르게 설정하면 메모리 접근 지연을 줄일 수 있습니다. 프로세스가 자신의 CPU와 가까운 메모리에 접근할 확률이 높아지기 때문에 성능 향상이 발생합니다.
🔹 성능 향상 추정치
성능 향상의 정도는 시스템 구조와 프로세스의 특성에 따라 다르지만, 일반적인 기대치를 제시할 수 있습니다.

경미한 성능 향상:

시스템이 단일 코어에서 실행되는 경우나 작은 프로세스들일 경우, CPU affinity가 큰 영향을 미치지 않을 수 있습니다. 이런 경우 성능 향상은 몇 퍼센트 내외일 수 있습니다.
중간 정도의 성능 향상:

멀티코어 시스템에서 여러 프로세스가 동시에 실행되고, 각 프로세스가 많은 계산을 수행하는 경우, CPU affinity를 설정하면 10~30% 정도 성능이 향상될 수 있습니다. 이 경우 캐시 지역성과 컨텍스트 스위칭 감소가 중요한 역할을 합니다.
큰 성능 향상:

CPU 코어 간 컨텍스트 스위칭이 많거나 NUMA 시스템에서 작업이 실행되는 경우, CPU affinity를 설정하면 30% 이상의 성능 향상을 기대할 수 있습니다. 특히 병렬 처리가 중요한 시스템에서 큰 효과를 볼 수 있습니다.
🔹 성능 향상을 극대화하는 전략
프로세스의 실행 패턴에 맞게 조정: CPU affinity를 잘 설정하면, 같은 코어에서 계속 실행되도록 하여 캐시 효율을 극대화하고, 컨텍스트 스위칭을 줄여 성능 향상을 얻을 수 있습니다.
CPU 코어 수와 프로세스 수에 비례: CPU 코어 수가 많고, 프로세스가 CPU의 자원을 효율적으로 사용하는 경우 성능 향상이 더 크게 나타날 수 있습니다.
NUMA 시스템에서의 최적화: NUMA 시스템에서는 각 프로세스가 가까운 메모리에 접근하도록 affinity를 설정하는 것이 성능을 크게 향상시킬 수 있습니다.
🔹 결론
CPU affinity 설정은 캐시 지역성과 컨텍스트 스위칭 감소에 의해 성능을 10~30% 정도 향상시킬 수 있습니다. 시스템의 프로세스 수가 많고 멀티코어 시스템에서 실행되는 경우, 특히 NUMA 아키텍처를 사용할 때 성능 향상 효과가 30% 이상 나올 수 있습니다.